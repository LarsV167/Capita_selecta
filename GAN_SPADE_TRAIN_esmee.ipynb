{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import UpsamplingNearest2d\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "import pdb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "#import u_net\n",
    "import utils\n",
    "import glob\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "from torchvision.models import vgg16\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import GAN_spade_esmee\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchsummary import summary\n",
    "\n",
    "#based on the jupyter notebook of website, import the following:\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpfull sources?\n",
    "https://opengtn.eu/wp-content/uploads/2022/02/Amirrajab_Intra-and-intersubject-synthesis-of-cardiac-MR-images-using-a-VAE-and-GAN-ISMRM2022.pdf  \n",
    "\n",
    "https://notebook.community/zklgame/CatEyeNets/test/GANs-PyTorch \n",
    "\n",
    "https://www.researchgate.net/figure/Discriminator-networks-architecture-All-convolution-layers-use-zero-padding-set-to-one_fig3_330470286 \n",
    "\n",
    "https://www.researchgate.net/figure/Figure-S36-The-label-input-method-for-the-generator-in-CcGAN_fig5_348834209\n",
    "\n",
    "https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd --> Open this file in incognito mode. Otherwise you won't have access\n",
    "\n",
    "Op het moment is het probleem dat de modellen niet goed leren. Dus de loss van de generator begint hoog maar daalt terwijl de loss van de generator juist laag begint en omhoog gaat. Eigenlijk zou je willen dat de loss van de generator juist omlaag gaat en de loss van de discriminator juist omhoog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path!\n",
    "\n",
    "random.seed(42)\n",
    "DATA_DIR = r\"C:\\Users\\esmee\\OneDrive - TU Eindhoven\\2022 - 2023\\Q3\\8DM20\\Project\\TrainingData\\TrainingData\"\n",
    "CHECKPOINTS_DIR = Path.cwd() / \"gan_model_weights\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Save results to tensorboard\n",
    "TENSORBOARD_LOGDIR_GAN = \"GAN_runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training settings and hyperparameters\n",
    "NO_VALIDATION_PATIENTS = 1 #I think we wanted to generate images on whole dataset so this should be 0?\n",
    "IMAGE_SIZE = [64, 64]\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 10\n",
    "DECAY_LR_AFTER = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "DISPLAY_FREQ = 10\n",
    "\n",
    "# dimension of VAE latent space\n",
    "Z_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reduce the\n",
    "def lr_lambda(the_epoch):\n",
    "    \"\"\"Function for scheduling learning rate\"\"\"\n",
    "    return (\n",
    "        1.0\n",
    "        if the_epoch < DECAY_LR_AFTER\n",
    "        else 1 - float(the_epoch - DECAY_LR_AFTER) / (N_EPOCHS - DECAY_LR_AFTER)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = [\n",
    "    path\n",
    "    for path in glob.glob(DATA_DIR+r\"\\p*[0-9]\")\n",
    "]\n",
    "\n",
    "random.shuffle(patients)\n",
    "\n",
    "# split in training/validation after shuffling\n",
    "partition = {\n",
    "    \"train\": patients[:-NO_VALIDATION_PATIENTS],\n",
    "    \"validation\": patients[-NO_VALIDATION_PATIENTS:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.ProstateMRDataset(partition[\"train\"], IMAGE_SIZE)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 64, 64])\n",
      "Labels batch shape: torch.Size([32, 1, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_features, train_labels = next(iter(dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0].squeeze()\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.imshow(label,cmap='gray',alpha=0.3)\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation data\n",
    "valid_dataset = utils.ProstateMRDataset(partition[\"validation\"], IMAGE_SIZE)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features, val_labels = next(iter(valid_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was used for training VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialise model, optimiser\n",
    "# vae_model = vae.VAE()\n",
    "# optimizer = torch.optim.Adam(vae_model.parameters(), lr=LEARNING_RATE)\n",
    "# # add a learning rate scheduler based on the lr_lambda function\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training loop\n",
    "# writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR)  # tensorboard summary\n",
    "# for epoch in range(N_EPOCHS):\n",
    "#     current_train_loss = 0.0\n",
    "#     current_valid_loss = 0.0\n",
    "\n",
    "#     for x_real, _ in tqdm(dataloader, position=0):\n",
    "#         # needed to zero gradients in each iteration\n",
    "#         optimizer.zero_grad()\n",
    "#         x_recon, mu, logvar = vae_model(x_real)  # forward pass\n",
    "#         loss = vae.vae_loss(x_real, x_recon, mu, logvar)\n",
    "#         current_train_loss += loss.item()\n",
    "#         loss.backward()  # backpropagate loss\n",
    "#         optimizer.step()  # update weights\n",
    "\n",
    "#     # write to tensorboard log\n",
    "#     writer.add_scalar(\"Loss/train\", current_train_loss / len(dataloader), epoch)\n",
    "\n",
    "#     scheduler.step()  # step the learning step scheduler\n",
    "\n",
    "#     # save examples of real/fake images\n",
    "#     if (epoch + 1) % 1 == 0:\n",
    "#         vae_model.eval()\n",
    "#         img_grid = make_grid(\n",
    "#             torch.cat((x_recon[:5], x_real[:5])), nrow=5, padding=12, pad_value=-1\n",
    "#         )\n",
    "#         writer.add_image(\n",
    "#             \"Real/fake_recon\",\n",
    "#             np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5,\n",
    "#             epoch + 1,\n",
    "#         )\n",
    "\n",
    "#         noise = torch.randn(10, Z_DIM)\n",
    "#         image_samples = vae_model.generator(noise)\n",
    "#         img_grid = make_grid(\n",
    "#             torch.cat((image_samples[:5], image_samples[5:])),\n",
    "#             nrow=5,\n",
    "#             padding=12,\n",
    "#             pad_value=-1,\n",
    "#         )\n",
    "#         writer.add_image(\n",
    "#             \"Samples\",\n",
    "#             np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5,\n",
    "#             epoch + 1,\n",
    "#         )\n",
    "\n",
    "#     vae_model.train()\n",
    "\n",
    "\n",
    "# torch.save(vae_model.state_dict(), CHECKPOINTS_DIR / \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN loss\n",
    "https://github.com/NVlabs/SPADE/blob/master/models/networks/loss.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GANLoss(nn.Module):\n",
    "#     def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n",
    "#                  tensor=torch.FloatTensor):\n",
    "#         super().__init__()\n",
    "#         self.real_label = target_real_label\n",
    "#         self.fake_label = target_fake_label\n",
    "#         self.real_label_var = None\n",
    "#         self.fake_label_var = None\n",
    "#         self.Tensor = tensor\n",
    "#         if use_lsgan:\n",
    "#             self.loss = nn.L1Loss()\n",
    "#         else:\n",
    "#             self.loss = nn.BCELoss()\n",
    "\n",
    "#     def get_target_tensor(self, input, target_is_real):\n",
    "#         target_tensor = None\n",
    "#         if target_is_real:\n",
    "#             create_label = ((self.real_label_var is None) or\n",
    "#                             (self.real_label_var.numel() != input.numel()))\n",
    "#             if create_label:\n",
    "#                 real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
    "#                 self.real_label_var = torch.tensor(real_tensor, requires_grad=False)\n",
    "#             target_tensor = self.real_label_var\n",
    "#         else:\n",
    "#             create_label = ((self.fake_label_var is None) or\n",
    "#                             (self.fake_label_var.numel() != input.numel()))\n",
    "#             if create_label:\n",
    "#                 fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
    "#                 self.fake_label_var = torch.tensor(fake_tensor, requires_grad=False)\n",
    "#             target_tensor = self.fake_label_var\n",
    "#         return target_tensor\n",
    "\n",
    "#     def __call__(self, input, target_is_real):        \n",
    "#         target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "#         return self.loss(input,target_tensor.to(torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an other version of the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://notebook.community/zklgame/CatEyeNets/test/GANs-PyTorch\n",
    "# #https://www.researchgate.net/figure/Figure-S36-The-label-input-method-for-the-generator-in-CcGAN_fig5_348834209\n",
    "# def bce_loss(input, target):\n",
    "#     \"\"\"\n",
    "#     Numerically stable version of the binary cross-entropy loss function.\n",
    "\n",
    "#     As per https://github.com/pytorch/pytorch/issues/751\n",
    "#     See the TensorFlow docs for a derivation of this formula:\n",
    "#     https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "\n",
    "#     Inputs:\n",
    "#     - input: PyTorch Variable of shape (N, ) giving scores.\n",
    "#     - target: PyTorch Variable of shape (N,) containing 0 and 1 giving targets.\n",
    "\n",
    "#     Returns:\n",
    "#     - A PyTorch Variable containing the mean BCE loss over the minibatch of input data.\n",
    "#     \"\"\"\n",
    "#     #neg_abs = - input.abs()\n",
    "#     #loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
    "#     BCE=torch.nn.BCELoss()\n",
    "#     loss=BCE(input,target)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discriminator_loss(logits_real, logits_fake):\n",
    "#     \"\"\"\n",
    "#     Computes the discriminator loss described above.\n",
    "    \n",
    "#     Inputs:\n",
    "#     - logits_real: PyTorch Variable of shape (N,) giving scores for the real data.\n",
    "#     - logits_fake: PyTorch Variable of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "#     Returns:\n",
    "#     - loss: PyTorch Variable containing (scalar) the loss for the discriminator.\n",
    "#     \"\"\"\n",
    "#     real_loss = bce_loss(logits_real, torch.ones(BATCH_SIZE,1))\n",
    "#     fake_loss = bce_loss(logits_fake, torch.zeros(BATCH_SIZE,1))\n",
    "#     loss = (real_loss + fake_loss) #Have a look at this ? correct?\n",
    "#     return loss\n",
    "\n",
    "# def generator_loss(logits_fake):\n",
    "#     \"\"\"\n",
    "#     Computes the generator loss described above.\n",
    "\n",
    "#     Inputs:\n",
    "#     - logits_fake: PyTorch Variable of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "#     Returns:\n",
    "#     - loss: PyTorch Variable containing the (scalar) loss for the generator.\n",
    "#     \"\"\"\n",
    "#     loss = bce_loss(logits_fake, torch.ones(BATCH_SIZE,1))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers = np.load(r'C:\\Users\\20181758\\Documents\\gan-checks-tf.npz')\n",
    "# dtype = torch.FloatTensor\n",
    "# from torch.autograd import Variable\n",
    "#Values for error should be 1e-8 or lower\n",
    "\n",
    "# def test_discriminator_loss(logits_real, logits_fake, d_loss_true):\n",
    "#     d_loss = discriminator_loss(Variable(torch.Tensor(logits_real)).type(dtype),\n",
    "#                                 Variable(torch.Tensor(logits_fake)).type(dtype)).data.cpu().numpy()\n",
    "#     print(\"Maximum error in d_loss: %g\", abs(1-(d_loss_true/ d_loss)))\n",
    "# \n",
    "# test_discriminator_loss(answers['logits_real'], answers['logits_fake'],\n",
    "#                         answers['d_loss_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_generator_loss(logits_fake, g_loss_true):\n",
    "#     g_loss = generator_loss(Variable(torch.Tensor(logits_fake)).type(dtype)).data.cpu().numpy()\n",
    "#     print(\"Maximum error in g_loss: %g\", 1-g_loss_true/ g_loss)\n",
    "\n",
    "# test_generator_loss(answers['logits_fake'], answers['g_loss_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are from the Args class that is created in the GAN_spade_esmee.py file\n",
    "import GAN_spade_esmee\n",
    "\n",
    "spade_filter = 64\n",
    "gen_input_size = 256\n",
    "gen_hidden_size = 128 * 16\n",
    "args = GAN_spade_esmee.Args(spade_filter, 3, 3, gen_input_size, gen_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the code we used before applying the SPADE code\n",
    "# #https://discuss.pytorch.org/t/weight-initilzation/157/6 \n",
    "# def weights_init(m): #I found this somewhere but maybe this wrong so try without\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "#     elif classname.find('BatchNorm') != -1:\n",
    "#         nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "#         nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ganloss.py of website\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n",
    "                 tensor=torch.FloatTensor):\n",
    "        super().__init__()\n",
    "        self.real_label = target_real_label\n",
    "        self.fake_label = target_fake_label\n",
    "        self.real_label_var = None\n",
    "        self.fake_label_var = None\n",
    "        self.Tensor = tensor\n",
    "        if use_lsgan:\n",
    "            self.loss = nn.L1Loss()\n",
    "        else:\n",
    "            self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        target_tensor = None\n",
    "        if target_is_real:\n",
    "            create_label = ((self.real_label_var is None) or\n",
    "                            (self.real_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
    "                self.real_label_var = torch.tensor(real_tensor, requires_grad=False)\n",
    "            target_tensor = self.real_label_var\n",
    "        else:\n",
    "            create_label = ((self.fake_label_var is None) or\n",
    "                            (self.fake_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
    "                self.fake_label_var = torch.tensor(fake_tensor, requires_grad=False)\n",
    "            target_tensor = self.fake_label_var\n",
    "        return target_tensor\n",
    "\n",
    "    def __call__(self, input, target_is_real):        \n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor.to(torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 # change this to a smaller number when trying the code!!!\n",
    "lr_gen = 0.0001\n",
    "lr_dis = 0.0004\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "gen = GAN_spade_esmee.Generator(args)\n",
    "dis = GAN_spade_esmee.Discriminator()\n",
    "\n",
    "gen = gen.to(device)\n",
    "dis = dis.to(device)\n",
    "\n",
    "#noise = torch.rand(4, 256)\n",
    "noise = torch.randn(32, 256)\n",
    "noise = noise.to(device)\n",
    "# not necessary to use these here, next cell is fine (what we also had in previous code)\n",
    "#img = img.to(device)\n",
    "#seg = seg.to(device)\n",
    "\n",
    "criterion = GANLoss()\n",
    "\n",
    "gen.apply(weights_init)\n",
    "dis.apply(weights_init)\n",
    "\n",
    "optim_gen = torch.optim.Adam(gen.parameters(), lr=lr_gen, betas=(0, 0.999))\n",
    "optim_dis = torch.optim.Adam(dis.parameters(), lr=lr_dis, betas=(0, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is the code we used before\n",
    "# epochs = 100\n",
    "# lr_gen = 0.0002\n",
    "# lr_dis = 0.00005\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# gen = GAN_Lars.Generator()\n",
    "# dis = GAN_Lars.Discriminator()\n",
    "\n",
    "# gen = gen.to(device)\n",
    "# dis = dis.to(device)\n",
    "\n",
    "# noise = torch.randn(32, 255)\n",
    "# noise = noise.to(device)\n",
    "# #criterion=GANLoss()\n",
    "\n",
    "# gen.apply(weights_init) #See what happens without\n",
    "# dis.apply(weights_init)\n",
    "\n",
    "# optim_gen = torch.optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.999))\n",
    "# optim_dis = torch.optim.Adam(dis.parameters(), lr=lr_dis, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [64] and output size of (4, 4). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m seg \u001b[38;5;241m=\u001b[39m seg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 17\u001b[0m fake_img \u001b[38;5;241m=\u001b[39m gen(noise, seg)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Fake Detection and Loss\u001b[39;00m\n\u001b[0;32m     20\u001b[0m pred_fake \u001b[38;5;241m=\u001b[39m dis(fake_img, seg)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\capita_selecta\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - TU Eindhoven\\2022 - 2023\\Q3\\8DM20\\Project\\Capita_selecta\\GAN_spade_esmee.py:81\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x, seg)\u001b[0m\n\u001b[0;32m     78\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n\u001b[0;32m     79\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(seg\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspade_resblk1(x, seg)\n\u001b[0;32m     82\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspade_resblk2(x, seg)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\capita_selecta\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - TU Eindhoven\\2022 - 2023\\Q3\\8DM20\\Project\\Capita_selecta\\spade_resblk.py:59\u001b[0m, in \u001b[0;36mSPADEResBlk.forward\u001b[1;34m(self, x, seg)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, seg):\n\u001b[0;32m     58\u001b[0m     x_skip \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 59\u001b[0m     x \u001b[38;5;241m=\u001b[39m relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspade1(x, seg))\n\u001b[0;32m     60\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m     61\u001b[0m     x \u001b[38;5;241m=\u001b[39m relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspade2(x, seg))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\capita_selecta\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - TU Eindhoven\\2022 - 2023\\Q3\\8DM20\\Project\\Capita_selecta\\spade_resblk.py:29\u001b[0m, in \u001b[0;36mSPADE.forward\u001b[1;34m(self, x, seg)\u001b[0m\n\u001b[0;32m     26\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(torch\u001b[38;5;241m.\u001b[39munsqueeze(std, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std\n\u001b[1;32m---> 29\u001b[0m seg \u001b[38;5;241m=\u001b[39m interpolate(seg, size\u001b[38;5;241m=\u001b[39m(H,W), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m seg \u001b[38;5;241m=\u001b[39m relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(seg))\n\u001b[0;32m     31\u001b[0m seg_gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_gamma(seg)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\capita_selecta\\Lib\\site-packages\\torch\\nn\\functional.py:3869\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   3868\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m-> 3869\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3870\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3871\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3872\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3873\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3874\u001b[0m \n\u001b[0;32m   3875\u001b[0m         )\n\u001b[0;32m   3876\u001b[0m     output_size \u001b[38;5;241m=\u001b[39m size\n\u001b[0;32m   3877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [64] and output size of (4, 4). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "# # code from website: watch out because this differs from our code. this code uses the seg instead of the img in both\n",
    "# # the discriminator and the generator\n",
    "# # see next cell for adjustment from seg to img in generator and discriminator!\n",
    "\n",
    "# img_lists = []\n",
    "# G_losses = []\n",
    "# D_losses = []\n",
    "# iters = 0\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     print(f'Epoch {epoch+1}/{epochs}')\n",
    "#     #for i, (img, seg) in enumerate(data['train']):\n",
    "#     #for i, (img, seg) in tqdm(dataloader):\n",
    "#     #for (img, seg) in tqdm(dataloader):\n",
    "#     for i, (img, seg) in enumerate(dataset):\n",
    "#     #for i, (img, seg) in enumerate(dataloader):\n",
    "#     #for i, (img, seg) in tqdm(dataset):\n",
    "#         img = img.to(device)\n",
    "#         seg = seg.to(device)\n",
    "        \n",
    "#         fake_img = gen(noise, seg)\n",
    "        \n",
    "#         # Fake Detection and Loss\n",
    "#         pred_fake = dis(fake_img, seg)\n",
    "#         loss_D_fake = criterion(pred_fake, False)\n",
    "        \n",
    "#         # Real Detection and Loss\n",
    "#         pred_real = dis(img, seg)\n",
    "#         loss_D_real = criterion(pred_real, True)\n",
    "        \n",
    "#         loss_G = criterion(pred_fake, True)\n",
    "#         loss_D = loss_D_fake + loss_D_real*0.5\n",
    "        \n",
    "#         # Backprop\n",
    "#         optim_gen.zero_grad()\n",
    "#         loss_G.backward(retain_graph=True)\n",
    "#         optim_gen.step()\n",
    "        \n",
    "#         optim_dis.zero_grad()\n",
    "#         loss_D.backward()\n",
    "#         optim_dis.step()\n",
    "        \n",
    "#         G_losses.append(loss_G.detach().cpu())\n",
    "#         D_losses.append(loss_D.detach().cpu())\n",
    "        \n",
    "#         if i%200 == 0:\n",
    "#             print(\"Iteration {}/{} started\".format(i+1, len(data['train'])))\n",
    "        \n",
    "#     print()\n",
    "#     if epoch%20 == 0:\n",
    "#         with torch.no_grad():\n",
    "#             img_lists.append(fake_img.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the code with the adjustment from seg to img in generator and discriminator!\n",
    "#still largely based on the code from the website \n",
    "# I did not yet check if the Generator and Discriminator in the GAN_spade_esmee.py file need to be adjusted due to this\n",
    "\n",
    "img_lists = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    #for i, (img, seg) in enumerate(data['train']):\n",
    "    #for i, (img, seg) in tqdm(dataloader):\n",
    "    #for (img, seg) in tqdm(dataloader):\n",
    "    for i, (img, seg) in enumerate(dataset):\n",
    "    #for i, (img, seg) in enumerate(dataloader):\n",
    "    #for i, (img, seg) in tqdm(dataset):\n",
    "        img = img.to(device)\n",
    "        seg = seg.to(device)\n",
    "        \n",
    "        fake_img = gen(noise, img)\n",
    "        \n",
    "        # Fake Detection and Loss\n",
    "        pred_fake = dis(fake_img, img)\n",
    "        loss_D_fake = criterion(pred_fake, False)\n",
    "        \n",
    "        # Real Detection and Loss\n",
    "        pred_real = dis(img, img)\n",
    "        loss_D_real = criterion(pred_real, True)\n",
    "        \n",
    "        loss_G = criterion(pred_fake, True)\n",
    "        loss_D = loss_D_fake + loss_D_real*0.5\n",
    "        \n",
    "        # Backprop\n",
    "        optim_gen.zero_grad()\n",
    "        loss_G.backward(retain_graph=True)\n",
    "        optim_gen.step()\n",
    "        \n",
    "        optim_dis.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optim_dis.step()\n",
    "        \n",
    "        G_losses.append(loss_G.detach().cpu())\n",
    "        D_losses.append(loss_D.detach().cpu())\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            print(\"Iteration {}/{} started\".format(i+1, len(data['train'])))\n",
    "        \n",
    "    print()\n",
    "    if epoch%20 == 0:\n",
    "        with torch.no_grad():\n",
    "            img_lists.append(fake_img.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen, 'gen.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dis, 'dis.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_losses = [x.item() for x in G_losses]\n",
    "d_losses = [x.item() for x in D_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Generator and Discriminator loss during training')\n",
    "plt.plot(g_losses, label=\"G\")\n",
    "plt.plot(d_losses, label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((20, 3, 64, 64))\n",
    "j = 0\n",
    "for i in range(5):\n",
    "    a = img_lists[i]\n",
    "    temp[j:j+4] = a\n",
    "    j += 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.from_numpy(temp)\n",
    "def imshow(img):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.001)\n",
    "    plt.savefig('results')\n",
    "    \n",
    "grid_img = torchvision.utils.make_grid(temp, nrow=4)\n",
    "imshow(grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def saveModels(epoch):\n",
    "#     torch.save(gen.state_dict(), CHECKPOINTS_DIR / \"gen.pth\")\n",
    "#     torch.save(dis.state_dict(), CHECKPOINTS_DIR / \"dis.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # add a learning rate scheduler based on the lr_lambda function\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optim_gen, lr_lambda)\n",
    "# adversarial_loss = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is the code we used before\n",
    "\n",
    "# img_lists = []\n",
    "# G_losses = []\n",
    "# D_losses = []\n",
    "# iters = 0\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR_GAN)  # tensorboard summary\n",
    "# valid = Variable(torch.FloatTensor(BATCH_SIZE, 1).fill_(1.0), requires_grad=False)\n",
    "# fake = Variable(torch.FloatTensor(BATCH_SIZE, 1).fill_(0.0), requires_grad=False)\n",
    "# adversarial_loss = torch.nn.MSELoss()\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     print(f'Epoch {epoch+1}/{epochs}')\n",
    "#     for (img, seg) in tqdm(dataloader):\n",
    "#         img = img.to(device)\n",
    "#         seg = seg.to(device)\n",
    "\n",
    "#         seg=seg.float()\n",
    "\n",
    "        \n",
    "#         fake_img = gen(noise, img) #remove seg if you want to train without labels\n",
    "        \n",
    "#         dis.trainable=True\n",
    "#         # Fake Detection and Loss\n",
    "#         pred_fake = dis(fake_img,img) #Also remove seg here if you want to train without labels\n",
    "       \n",
    "\n",
    "#         # Real Detection and Loss\n",
    "       \n",
    "#         pred_real = dis(img,img)\n",
    "        \n",
    "        \n",
    "#         loss = discriminator_loss(pred_real,pred_fake)\n",
    "#         #loss_fake=adversarial_loss(pred_fake,seg)        \n",
    "#         loss_D=loss\n",
    "        \n",
    "        \n",
    "#         optim_dis.zero_grad()\n",
    "#         loss_D.backward()\n",
    "#         optim_dis.step()\n",
    "        \n",
    "        \n",
    "#         D_losses.append(loss_D.detach().cpu())\n",
    "        \n",
    "#         dis.trainable = False\n",
    "        \n",
    "#         loss_G =  generator_loss(pred_fake)\n",
    "        \n",
    "#         optim_gen.zero_grad()\n",
    "\n",
    "#         #loss_G.backward(retain_graph=True)\n",
    "        \n",
    "#         optim_gen.step()\n",
    "\n",
    "#         G_losses.append(loss_G.detach().cpu())\n",
    "#         # if i%200 == 0:\n",
    "#         #     print(\"Iteration {}/{} started\".format(i+1, len(dataloader)))\n",
    "#     writer.add_scalar(\"Loss dis\", loss_D / len(dataloader), epoch)\n",
    "#     writer.add_scalar(\"Loss gen\", loss_G / len(dataloader), epoch)  \n",
    "#     scheduler.step()   \n",
    "#     print(\"G_loss= \",loss_G, \" : D_Loss= \", loss_D)\n",
    "\n",
    "    \n",
    "#     if (epoch + 1) % 1 == 0: #This is for the tensorboard\n",
    "#         gen.eval()\n",
    "#         img_grid = make_grid(\n",
    "#             torch.cat((fake_img[:5], img[:5])), nrow=5, padding=12, pad_value=-1\n",
    "#         )\n",
    "#         writer.add_image(\n",
    "#             \"Real/fake_recon\",\n",
    "#             np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5,\n",
    "#             epoch + 1,\n",
    "#         )\n",
    "        \n",
    "#     if epoch%2 == 0:\n",
    "#         with torch.no_grad():\n",
    "#             img_lists.append(fake_img.detach().cpu().numpy())\n",
    "#     #Plot results between epochs to see if it generates something\n",
    "#     if epoch%2==0:\n",
    "#         for i in range(25):\n",
    "#             plt.subplot(5, 5, i+1)\n",
    "#             img=img_lists[0].squeeze()\n",
    "#             #print(img.shape)\n",
    "#             plt.imshow(img[i], interpolation='nearest', cmap='gray_r')\n",
    "#             plt.axis('off')\n",
    "#         plt.tight_layout()\n",
    "#         plt.suptitle('Epoch {}'.format(epoch))\n",
    "#         plt.show()\n",
    "#         saveModels(epoch)\n",
    "                \n",
    "#     iters+=1\n",
    "# torch.save(gen.state_dict(), CHECKPOINTS_DIR / \"gan.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Sigmoid()\n",
    "# loss = nn.BCELoss()\n",
    "# input = torch.randn(3, requires_grad=True)\n",
    "# target = torch.empty(3).random_(2)\n",
    "# output = loss(m(input), target)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(0,370),G_losses)\n",
    "# plt.plot(np.arange(0,370),D_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img = img_lists[0].squeeze()\n",
    "# print(img.shape)\n",
    "# plt.imshow(img[30],cmap= 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir='GAN_runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gener=torch.load(r\"C:\\Users\\20181758\\Documents\\master\\Q3\\8DM20\\8DM20\\segmentation-solution\\gan_model_weights\\gen.pth\")\n",
    "# gen.load_state_dict(gener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img,lab=tqdm(valid_dataloader)\n",
    "# pres=[]\n",
    "# for (img, seg) in tqdm(valid_dataloader):\n",
    "#     lab = seg.to(device).type(torch.Tensor)\n",
    "#     y_pred = gen(noise,lab)\n",
    "#     pres.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image=pres[0].squeeze()\n",
    "# plt.imshow(image[5].detach().numpy(), interpolation='nearest', cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training loop\n",
    "# writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR)  # tensorboard summary\n",
    "# for epoch in range(N_EPOCHS):\n",
    "#     current_train_loss = 0.0\n",
    "#     current_valid_loss = 0.0\n",
    "\n",
    "#     for x_real, _ in tqdm(dataloader, position=0):\n",
    "#         # needed to zero gradients in each iteration\n",
    "#         optimizer.zero_grad()\n",
    "#         x_recon, mu, logvar = vae_model(x_real)  # forward pass\n",
    "#         loss = vae.vae_loss(x_real, x_recon, mu, logvar)\n",
    "#         current_train_loss += loss.item()\n",
    "#         loss.backward()  # backpropagate loss\n",
    "#         optimizer.step()  # update weights\n",
    "\n",
    "#     # write to tensorboard log\n",
    "#     writer.add_scalar(\"Loss/train\", current_train_loss / len(dataloader), epoch)\n",
    "\n",
    "#     scheduler.step()  # step the learning step scheduler\n",
    "\n",
    "    # save examples of real/fake images\n",
    "    # if (epoch + 1) % 1 == 0:\n",
    "    #     vae_model.eval()\n",
    "    #     img_grid = make_grid(\n",
    "    #         torch.cat((x_recon[:5], x_real[:5])), nrow=5, padding=12, pad_value=-1\n",
    "    #     )\n",
    "    #     writer.add_image(\n",
    "    #         \"Real/fake_recon\",\n",
    "    #         np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5,\n",
    "    #         epoch + 1,\n",
    "    #     )\n",
    "\n",
    "#         noise = torch.randn(10, Z_DIM)\n",
    "#         image_samples = vae_model.generator(noise)\n",
    "#         img_grid = make_grid(\n",
    "#             torch.cat((image_samples[:5], image_samples[5:])),\n",
    "#             nrow=5,\n",
    "#             padding=12,\n",
    "#             pad_value=-1,\n",
    "#         )\n",
    "#         writer.add_image(\n",
    "#             \"Samples\",\n",
    "#             np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5,\n",
    "#             epoch + 1,\n",
    "#         )\n",
    "\n",
    "#     vae_model.train()\n",
    "\n",
    "\n",
    "# torch.save(vae_model.state_dict(), CHECKPOINTS_DIR / \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1%50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn.functional import leaky_relu, interpolate\n",
    "# from torch.nn.utils import spectral_norm\n",
    "# import numpy as np\n",
    "# l1_loss = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "# class Block(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Class for the basic convolutional building block\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, in_ch, out_ch):\n",
    "#         \"\"\"\n",
    "#         Constructor.\n",
    "#         :param in_ch: number of input channels to the block\n",
    "#         :param out_ch: number of output channels of the block\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "#         self.relu = nn.LeakyReLU(0.2, inplace=True)  # leaky ReLU\n",
    "#         self.bn1 = nn.BatchNorm2d(out_ch)  # batch normalisation\n",
    "#         self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_ch) #0.8 weghalen\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Returns the output of a forward pass of the block\n",
    "#         :param x: the input tensor\n",
    "#         :return: the output tensor of the block\n",
    "#         \"\"\"\n",
    "#         # a block consists of two convolutional layers\n",
    "#         # with ReLU activations\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.bn1(x)  # batch normalisation\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.bn2(x)\n",
    "\n",
    "#         return x\n",
    "    \n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Class for the generator part of the GAN.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, z_dim=256, chs=(256, 128, 64, 32), h=8, w=8):\n",
    "#         \"\"\"\n",
    "#         Constructor.\n",
    "#         :param chs: tuple giving the number of input channels of each block in the decoder\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.chs = chs\n",
    "#         self.h = h  # height of image at lowest resolution level\n",
    "#         self.w = w  # width of image at lowest resolution level\n",
    "#         self.z_dim = z_dim  # dimension of latent space\n",
    "#         self.proj_z = nn.Linear(\n",
    "#             self.z_dim, self.chs[0] * self.h * self.w\n",
    "#         )  # fully connected layer on latent space\n",
    "#         self.reshape = lambda x: torch.reshape(\n",
    "#             x, (-1, self.chs[0], self.h, self.w)\n",
    "#         )  # reshaping\n",
    "\n",
    "#         self.upconvs = nn.ModuleList(\n",
    "#             [nn.ConvTranspose2d(chs[i], chs[i], 2, 2) for i in range(len(chs) - 1)]\n",
    "#         )\n",
    "\n",
    "#         self.dec_blocks = nn.ModuleList(\n",
    "#             [Block(chs[i], chs[i + 1]) for i in range(len(chs) - 1)]\n",
    "#         )\n",
    "#         self.proj_o = nn.Sequential(\n",
    "#             nn.Conv2d(self.chs[-1], 1, kernel_size=3, padding=1),\n",
    "#             nn.Tanh(),\n",
    "#         )  # output layer\n",
    "\n",
    "#     def forward(self, z,seg):\n",
    "#         \"\"\"\n",
    "#         Returns the output of the decoder part of the VAE\n",
    "#         :param x: input tensor to the generator\n",
    "#         \"\"\"\n",
    "       \n",
    "#         b, c, h, w = seg.size()\n",
    "#         x = self.proj_z(z)  # fully connected layer\n",
    "#         x = self.reshape(x)  # reshape to image dimension\n",
    "#         seg=interpolate(seg, size=(seg.size(2)//8, seg.size(3)//8), mode='bilinear')\n",
    "#         print(seg.size())\n",
    "#         print(x.size())\n",
    "#         #print(seg.size(),x.size())\n",
    "#         x=torch.cat((x,seg),dim=0)\n",
    "#         for i in range(len(self.chs) - 1):\n",
    "#             x = self.upconvs[i](x)\n",
    "#             x = self.dec_blocks[i](x)\n",
    "#         x = self.proj_o(x)  # output layer\n",
    "#         return x\n",
    "\n",
    "# # class Generator(nn.Module):\n",
    "# #     def __init__(self):\n",
    "# #         super(Generator, self).__init__()\n",
    "\n",
    "\n",
    "# #         def block(in_feat, out_feat, normalize=True):\n",
    "# #             layers = [nn.Linear(in_feat, out_feat)]\n",
    "# #             if normalize:\n",
    "# #                 layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "# #             layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "# #             return layers\n",
    "\n",
    "# #         self.model = nn.Sequential(\n",
    "# #             *block(256, 128, normalize=False),\n",
    "# #             *block(128, 256),\n",
    "# #             nn.Linear(32, 256*8*8),\n",
    "# #             nn.Tanh())\n",
    "# #         self.reshape = lambda x: torch.reshape(\n",
    "# #             x, (32, -1, 8, 8)\n",
    "# #         )  # reshaping\n",
    "\n",
    "\n",
    "# #     def forward(self, noise, labels):\n",
    "# #         # Concatenate label embedding and image to produce input\n",
    "# #         noise=self.reshape(noise)\n",
    "# #         print(noise.size())\n",
    "# #         print(labels.size())\n",
    "# #         seg=interpolate(labels, size=(labels.size(2)//8, labels.size(3)//8), mode='bilinear')\n",
    "# #         gen_input = torch.cat((seg, noise), dim=1)\n",
    "# #         img = self.model(gen_input)\n",
    "# #         img = img.view(img.size(0), (1,64,64))\n",
    "# #         return img\n",
    "\n",
    "# # class Generator(nn.Module):\n",
    "# #     def __init__(self):\n",
    "# #         super().__init__()\n",
    "# #         self.model = nn.Sequential(\n",
    "# #             nn.Linear(256, 256*8*8),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Linear(64, 128),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Linear(128, 256),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Linear(128, 256),\n",
    "# #             nn.ReLU(),\n",
    "# #             nn.Linear(128, 256),\n",
    "# #         )\n",
    "\n",
    "# #     def forward(self, x):\n",
    "# #         output = self.model(x)\n",
    "# #         return output\n",
    "        \n",
    "# def custom_model1(in_chan, out_chan):\n",
    "#     return nn.Sequential(\n",
    "#         spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(3,3), stride=2, padding=1)),\n",
    "#         nn.LeakyReLU(inplace=True)\n",
    "#     )\n",
    "\n",
    "# def custom_model2(in_chan, out_chan, stride=2):\n",
    "#     return nn.Sequential(\n",
    "#         spectral_norm(nn.Conv2d(in_chan, out_chan, kernel_size=(3,3), stride=stride, padding=1)),\n",
    "#         nn.InstanceNorm2d(out_chan),\n",
    "#         nn.LeakyReLU(inplace=True)\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layer1 = custom_model1(32, 64)\n",
    "#         self.layer2 = custom_model2(64, 128)\n",
    "#         self.layer3 = custom_model2(128, 256,stride=1)\n",
    "#         #self.layer4 = custom_model2(256, 512, stride=1)\n",
    "#         self.inst_norm = nn.InstanceNorm2d(256)\n",
    "#         self.conv = spectral_norm(nn.Conv2d(256, 1, kernel_size=(3,3), padding=1))\n",
    "#         # self.out=nn.Sequential(\n",
    "#         #     self.conv,\n",
    "#         #     nn.Sigmoid(),\n",
    "#         # )  # output layer\n",
    "\n",
    "#         self.out = nn.Sequential(nn.Flatten(1), nn.Linear(256,1), nn.Sigmoid())\n",
    "#     def forward(self, img, seg):\n",
    "#        # x = torch.cat((img.view(img.size(0), -1),\n",
    "#                           #seg.view(seg.size(0), -1)), dim=1)\n",
    "#         x = torch.cat(( img.detach(),seg.detach()), dim=1)\n",
    "#         x = x.view(-1, 32, 64, 64)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         #x = self.layer4(x)\n",
    "#         x = leaky_relu(self.inst_norm(x))\n",
    "#         x = self.out(x)\n",
    "#         return x\n",
    "    \n",
    "\n",
    "# def get_noise(n_samples, z_dim, device=\"cpu\"):\n",
    "#     \"\"\"\n",
    "#     Function for creating noise vectors: Given the dimensions (n_samples, z_dim),\n",
    "#     creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "#     Parameters:\n",
    "#         n_samples: the number of samples to generate, a scalar\n",
    "#         z_dim: the dimension of the noise vector, a scalar\n",
    "#         device: the device type\n",
    "#     \"\"\"\n",
    "#     return torch.randn(n_samples, z_dim, device=device)\n",
    "\n",
    "\n",
    "# def sample_z(mu, logvar):\n",
    "#     \"\"\"\n",
    "#     Samples noise vector with reparameterization trick.\n",
    "#     \"\"\"\n",
    "#     eps = torch.randn(mu.size(), device=mu.device).to(mu.dtype)\n",
    "#     return (logvar / 2).exp() * eps + mu\n",
    "\n",
    "\n",
    "# def kld_loss(mu, logvar):\n",
    "#     \"\"\"\n",
    "#     Returns KLD loss\n",
    "#     \"\"\"\n",
    "#     return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "\n",
    "# def vae_loss(inputs, recons, mu, logvar):\n",
    "#     \"\"\"\n",
    "#     Returns VAE loss, sum of reconstruction and KLD loss\n",
    "#     \"\"\"\n",
    "#     return l1_loss(inputs, recons) + kld_loss(mu, logvar)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
