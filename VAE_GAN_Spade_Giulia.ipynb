{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import UpsamplingNearest2d\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.distributions import Normal\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "import pdb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "#import u_net\n",
    "import utils\n",
    "import glob\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "from torchvision.models import vgg16\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import os\n",
    "#import vae\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "#import GAN_Lars\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "DATA_DIR = r\"C:\\Users\\20191679\\Documents\\Master\\CS_image_analysis\\TrainingData\\TrainingData\"\n",
    "CHECKPOINTS_DIR = Path.cwd() / \"vae_spade_weights\"\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Save results to tensorboard\n",
    "TENSORBOARD_LOGDIR_GAN = \"GAN_runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training settings and hyperparameters\n",
    "NO_VALIDATION_PATIENTS = 1 #I think we wanted to generate images on whole dataset so this should be 0?\n",
    "IMAGE_SIZE = [64, 64]\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 10\n",
    "DECAY_LR_AFTER = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "DISPLAY_FREQ = 10\n",
    "\n",
    "# dimension of VAE latent space\n",
    "Z_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(the_epoch):\n",
    "    \"\"\"Function for scheduling learning rate\"\"\"\n",
    "    return (\n",
    "        1.0\n",
    "        if the_epoch < DECAY_LR_AFTER\n",
    "        else 1 - float(the_epoch - DECAY_LR_AFTER) / (N_EPOCHS - DECAY_LR_AFTER)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p120',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p116',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p108',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p125',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p127',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p117',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p133',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p109',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p115',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p102',\n",
       " 'C:\\\\Users\\\\20191679\\\\Documents\\\\Master\\\\CS_image_analysis\\\\TrainingData\\\\TrainingData\\\\p107']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients = [\n",
    "    path\n",
    "    for path in glob.glob(DATA_DIR+r\"\\p*\")\n",
    "]\n",
    "\n",
    "random.shuffle(patients)\n",
    "\n",
    "# split in training/validation after shuffling\n",
    "partition = {\n",
    "    \"train\": patients[:-NO_VALIDATION_PATIENTS],\n",
    "    \"validation\": patients[-NO_VALIDATION_PATIENTS:],\n",
    "}\n",
    "\n",
    "partition['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.ProstateMRDataset(partition[\"train\"], IMAGE_SIZE)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 64, 64])\n",
      "Labels batch shape: torch.Size([32, 1, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAM8UlEQVR4nO3dX4xc5X3G8e9TG5c0oTGG1LIw1CBQEBfBRBYFBVWEishNo8AFQkSp5Faoe5NKRK2UQCu1TaVK5SYEqVUlC2h80QYoaWLERYnjELVXBhOgMTgOTgrClsGtAKXpBarJrxdztlpW+2c8M2dml/f7kUY75+zMnJ995pn3Pe+cPW+qCknvf7806wIkTYdhlxph2KVGGHapEYZdaoRhlxoxVtiT7E5yLMnxJHdPqihJk5dRv2dPsgH4MXAzcAJ4BvhcVb00ufIkTcrGMZ57LXC8qn4KkORh4BZg2bAn8QweqWdVlaXWj9ONvwh4bcHyiW6dpDVonJZ9KEnmgLm+tyNpZeOE/SRw8YLl7d2696iqvcBesBsvzdI43fhngCuSXJpkE3AH8PhkypI0aSO37FV1JskfAk8CG4CHqurFiVUmaaJG/uptpI3ZjZd618dovKR1xLBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41YtWwJ3koyekkRxas25LkQJKXu5/n91umpHEN07J/Hdi9aN3dwMGqugI42C1LWsNWDXtV/Svw5qLVtwD7uvv7gFsnW5akSRv1mH1rVZ3q7r8ObJ1QPZJ6MvKUzfOqqlaanTXJHDA37nYkjWfUlv2NJNsAup+nl3tgVe2tql1VtWvEbUmagFHD/jiwp7u/B9g/mXIk9SVVy/bABw9IvgHcCFwIvAH8OfBt4FHgEuBV4PaqWjyIt9RrrbwxSWOrqiy1ftWwT5Jhl/q3XNg9g05qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRY0/sqPef5SYOSZace0DrxKote5KLkzyV5KUkLya5q1u/JcmBJC93P8/vv1xJoxpmrrdtwLaq+kGS84BngVuB3wPerKq/TnI3cH5VfXmV13L6p3XAln19m9hcb0n2A3/T3W6sqlPdB8L3q+qjqzzXsK8RU57jb2rb0oTmekuyA7gGOARsrapT3a9eB7aOU6Ckfg09QJfkQ8A3gS9W1c8WflpXVS3XaieZA+bGLVTSeIbqxic5B3gCeLKqvtqtO4bd+HXLbvz718jd+Az21IPA0fmgdx4H9nT39wD7xy1Sk1VVy95mVYdmZ5jR+BuAfwN+CPyiW/0nDI7bHwUuAV4Fbq+qN1d5Lff2FK3FcNnK929io/HjMOzTZdjbtFzYPYNunVuLgV7JSvX6QdAvz42XGmHYpUbYjV+H1lvXXWuDLbvUCMMuNcKwS43wmF1rxuKxCL+KmyxbdqkRhl1qhN34daDVr9oW/rvt0o/Pll1qhGGXGmHYpUZ4zL5GtXqcvhyP38dnyy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wjPo1hDPmlOfhpnr7dwkTyd5IcmLSb7Srb80yaEkx5M8kmRT/+VKGtUw3fh3gJuq6mpgJ7A7yXXAvcB9VXU58BZwZ29VShrbqmGvgZ93i+d0twJuAh7r1u8Dbu2jQEmTMdQAXZINSZ4HTgMHgJ8Ab1fVme4hJ4CLeqlQ0kQMFfaqereqdgLbgWuBK4fdQJK5JIeTHB6tREmTcFZfvVXV28BTwPXA5iTzo/nbgZPLPGdvVe2qql3jFCppPMOMxn8kyebu/geAm4GjDEJ/W/ewPcD+nmqUNAFZ7bvdJB9jMAC3gcGHw6NV9ZdJLgMeBrYAzwG/W1XvrPJafpG8Ar9nH45XqllZVS35H7Rq2CfJsK/MsA/HsK9subB7uqzUCMMuNcKwS43wD2HWkIXHoh6/v5fH6eOzZZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRni67BrlqbOaNFt2qRGGXWqEYZcaYdilRhh2qRGOxq8Diy/c4Oi8RmHLLjXCsEuNMOxSIzxmX4dWuvjiKMfzXsyxDUO37N20zc8leaJbvjTJoSTHkzySZFN/ZUoa19l04+9iMKHjvHuB+6rqcuAt4M5JFiZpsoYKe5LtwO8AD3TLAW4CHusesg+4tYf6dJaSnPVNbRi2Zf8a8CXgF93yBcDbVXWmWz4BXDTZ0iRN0jDzs38GOF1Vz46ygSRzSQ4nOTzK8yVNxjCj8Z8APpvk08C5wK8C9wObk2zsWvftwMmlnlxVe4G94JTN0iyt2rJX1T1Vtb2qdgB3AN+rqs8DTwG3dQ/bA+zvrUpJYxvnpJovA3+U5DiDY/gHJ1OSpD5kmn9UYTde6l9VLfkVi6fLSo0w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40YZmJHkrwC/DfwLnCmqnYl2QI8AuwAXgFur6q3+ilT0rjOpmX/ZFXtrKpd3fLdwMGqugI42C1LWqPG6cbfAuzr7u8Dbh27Gkm9GTbsBXwnybNJ5rp1W6vqVHf/dWDrxKuTNDFDHbMDN1TVySS/BhxI8qOFv6yqWm6G1u7DYW6p30manrOesjnJXwA/B/4AuLGqTiXZBny/qj66ynOdslnq2chTNif5YJLz5u8DnwKOAI8De7qH7QH2T6ZUSX1YtWVPchnwrW5xI/CPVfVXSS4AHgUuAV5l8NXbm6u8li271LPlWvaz7saPw7BL/Ru5Gy/p/cGwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNWKosCfZnOSxJD9KcjTJ9Um2JDmQ5OXu5/l9FytpdMO27PcD/1JVVwJXA0eBu4GDVXUFcLBblrRGDTOx44eB54HLasGDkxzDKZulNWecud4uBf4T+PskzyV5oJu6eWtVneoe8zqwdTKlSurDMGHfCHwc+Luqugb4HxZ12bsWf8lWO8lcksNJDo9brKTRDRP2E8CJqjrULT/GIPxvdN13up+nl3pyVe2tql1VtWsSBUsazaphr6rXgdeSzB+P/xbwEvA4sKdbtwfY30uFkiZi1QE6gCQ7gQeATcBPgd9n8EHxKHAJ8Cpwe1W9ucrrOEAn9Wy5Abqhwj4phl3q3zij8ZLeBwy71AjDLjXCsEuNMOxSIwy71AjDLjVi45S3918MTsC5sLs/S2uhBrCOxazjvc62jl9f7hdTPanm/zeaHJ71ufJroQbrsI5p1mE3XmqEYZcaMauw753RdhdaCzWAdSxmHe81sTpmcswuafrsxkuNmGrYk+xOcizJ8SRTuxptkoeSnE5yZMG6qV8KO8nFSZ5K8lKSF5PcNYtakpyb5OkkL3R1fKVbf2mSQ93+eSTJpj7rWFDPhu76hk/Mqo4kryT5YZLn5y+hNqP3SG+XbZ9a2JNsAP4W+G3gKuBzSa6a0ua/DuxetG4Wl8I+A/xxVV0FXAd8ofs/mHYt7wA3VdXVwE5gd5LrgHuB+6rqcuAt4M6e65h3F4PLk8+bVR2frKqdC77qmsV7pL/LtlfVVG7A9cCTC5bvAe6Z4vZ3AEcWLB8DtnX3twHHplXLghr2AzfPshbgV4AfAL/B4OSNjUvtrx63v717A98EPAFkRnW8Aly4aN1U9wvwYeA/6MbSJl3HNLvxFwGvLVg+0a2blZleCjvJDuAa4NAsaum6zs8zuFDoAeAnwNtVdaZ7yLT2z9eALwG/6JYvmFEdBXwnybNJ5rp1094vvV623QE6Vr4Udh+SfAj4JvDFqvrZLGqpqneraieDlvVa4Mq+t7lYks8Ap6vq2Wlvewk3VNXHGRxmfiHJby785ZT2y1iXbV/NNMN+Erh4wfL2bt2sDHUp7ElLcg6DoP9DVf3zLGsBqKq3gacYdJc3J5n/e4lp7J9PAJ9N8grwMIOu/P0zqIOqOtn9PA18i8EH4LT3y1iXbV/NNMP+DHBFN9K6CbiDweWoZ2Xql8JOEuBB4GhVfXVWtST5SJLN3f0PMBg3OMog9LdNq46quqeqtlfVDgbvh+9V1eenXUeSDyY5b/4+8CngCFPeL9X3Zdv7HvhYNNDwaeDHDI4P/3SK2/0GcAr4XwafnncyODY8CLwMfBfYMoU6bmDQBft3BvPnPd/9n0y1FuBjwHNdHUeAP+vWXwY8DRwH/gn45SnuoxuBJ2ZRR7e9F7rbi/PvzRm9R3YCh7t9823g/EnV4Rl0UiMcoJMaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWrE/wETB78EONToMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)\n",
      "img: tensor([[-0.2423, -0.0737,  0.0330,  ...,  0.4274,  0.3837,  0.3575],\n",
      "        [ 0.0223,  0.2467,  0.3483,  ...,  0.1457,  0.3493,  0.2652],\n",
      "        [-0.6800, -0.8333, -0.5341,  ...,  0.1063,  0.3852,  0.5044],\n",
      "        ...,\n",
      "        [ 2.3757,  1.9672, -0.2122,  ...,  1.3052,  2.2390,  2.1840],\n",
      "        [-0.6700, -0.5730, -0.9163,  ...,  1.8955,  0.7381, -0.1023],\n",
      "        [-0.5444, -0.7394, -0.7713,  ...,  0.0360,  0.1093,  0.2939]])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_features, train_labels = next(iter(dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0].squeeze()\n",
    "\n",
    "#plt.imshow(img, cmap=\"gray\")\n",
    "plt.imshow(label,cmap='gray',)\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"img: {img}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 64, 64])\n",
      "torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\nn\\functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD4CAYAAAAjDTByAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMvElEQVR4nO3df+xd9V3H8edLCkMYQpHIOsANFkKiiwppCFsIkqDYIVkxWQyL0zKWNERRMC6kG4kuJsbM+duYmYoomoZNGWxkAQfi4vxDKqVSaIFBQQathW7DwHR/sLq3f9xT/fbL93u/3957zu23/TwfyTf33HM+957393O/r+8599x7zidVhaT2fM+RLkDSkWH4pUYZfqlRhl9qlOGXGrVqlitL4kcL0sCqKstp55ZfapThlxpl+KVGTRX+JOuSfDXJ7iSb+ipK0vAy6dd7kxwHPAP8JLAHeAT4YFU9OeYxHvCTBjaLA34XA7ur6vmqegP4DLB+iueTNEPThP8s4KU59/d08w6RZGOSbUm2TbEuST0b/HP+qtoMbAZ3+6WVZJot/17gnDn3z+7mSToKTBP+R4Dzk5yb5ATgWuDefsqSNLSJd/ur6kCSG4EvAccBt1fVrt4qkzSoiT/qm2hlvueXBud3+yWNZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRk0c/iTnJPlykieT7EpyU5+FSRrWNMN1rQHWVNX2JKcAjwLXOFyXdGQNfg2/qtpXVdu76W8BT7HAiD2SVqZeRuxJ8k7gQmDrAss2Ahv7WI+k/kx96e4kbwX+Cfitqrp7ibbu9ksDm8mlu5McD3wO2LJU8CWtLNMc8AtwB/BqVd28zMe45ZcGttwt/zThvxT4Z+AJ4Lvd7I9X1X1jHmP4pYENHv5JGH5peA7XJWkswy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81aurwJzkuyb8l+WIfBUmajT62/DcxGq1H0lFk2uv2nw38NHBbP+VImpVpt/x/CNzC/1+6W9JRYpohuq8G9lfVo0u025hkW5Jtk65LUv+mGbTjt4GfBw4AJwLfB9xdVR8a8xiv2y8NbKaDdiS5HPhoVV29RDvDLw3MQTskjeVwXdIxxi2/pLFWHekCNBuz3MMDWLt27WE/5tFHx35wpJ655ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4Zca5fn8R6FZn6E3K5OcCQieDTif5/NLGsvwS40y/FKjph2x57QkdyV5OslTSd7TV2GShjXtZbz+CPj7qvpAkhOAk3qoSdIMTBz+JKcClwHXAVTVG8Ab/ZQlaWjT7PafC3wd+MtuiO7bkpw8v5HDdUkr0zThXwVcBHy6qi4E/hvYNL9RVW2uqrVVNdmHuJIGMU349wB7qmprd/8uRv8MJB0FJg5/Vb0MvJTkgm7WFcCTvVQlaXDTHu3/ZWBLd6T/eeDD05ckaRamCn9VPQb4Xl46CnlizxF0rJ6gM2vJss5jaYYn9kgay/BLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81atrz+TWFSc9GO1bPBvTsvNlyyy81yvBLjTL8UqOmHa7rV5PsSrIzyZ1JTuyrMEnDmjj8Sc4CfgVYW1XvBo4Dru2rMEnDmna3fxXwvUlWMRqn7z+mL0nSLExz3f69wO8CLwL7gNeq6oH57RyuS1qZptntXw2sZzRm39uBk5N8aH47h+uSVqZpdvt/Avj3qvp6VX0HuBt4bz9lSRraNOF/EbgkyUkZfTXrCuCpfsqSNLRp3vNvZTQ453bgie65NvdUl6SBOWLPUcjv9mscR+yRNJbhlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxrlcF1HIU99VR/c8kuNMvxSowy/1Kglw5/k9iT7k+ycM+/0JA8meba7XT1smZL6tpwt/18B6+bN2wQ8VFXnAw919yUdRZYMf1V9BXh13uz1wB3d9B3ANf2WJWlok37Ud2ZV7eumXwbOXKxhko3AxgnXI2kgU3/OX1U17pLcVbWZ7nr+XrpbWjkmPdr/SpI1AN3t/v5KkjQLk4b/XmBDN70B+EI/5UialSVH7ElyJ3A5cAbwCvAbwOeBvwV+EPga8LNVNf+g4ELP5W6/NLDljtjjcF3SMcbhuiSNZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRk06XNenkjyd5PEk9yQ5bdAqJfVu0uG6HgTeXVU/AjwDfKznuiQNbKLhuqrqgao60N19GDh7gNokDaiP9/zXA/cvtjDJxiTbkmzrYV2SejLVcF1JbgUOAFsWa+NwXdLKNHH4k1wHXA1cUbO8+L+kXkwU/iTrgFuAH6+qb/dbkqRZmHS4ro8BbwG+2TV7uKpuWHJl7vZLg3O4LqlRDtclaSzDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzVqouG65iz7tSSV5IxhypM0lEmH6yLJOcCVwIs91yRpBiYarqvzB4wu3+1FOaWj0KTX7V8P7K2qHcn4C4Um2QhsnGQ9koZz2OFPchLwcUa7/EtyuC5pZZrkaP+7gHOBHUleYDRC7/Ykb+uzMEnDOuwtf1U9AfzAwfvdP4C1VfWNHuuSNLDlfNR3J/AvwAVJ9iT5yPBlSRqaw3VJxxiH65I0luGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUZNdAHPKXwD+Noiy87olh9p1nEo6zjUSq/jHct9gplezGOcJNuqaq11WId1zKYOd/ulRhl+qVErKfybj3QBHes4lHUc6pipY8W855c0Wytpyy9phgy/1KiZhj/JuiRfTbI7yaYFlr8lyWe75VuTvHOAGs5J8uUkTybZleSmBdpcnuS1JI91P7/edx1z1vVCkie69WxbYHmS/HHXJ48nuajn9V8w5/d8LMnrSW6e12aw/khye5L9SXbOmXd6kgeTPNvdrl7ksRu6Ns8m2TBAHZ9K8nTX7/ckOW2Rx459DXuo4xNJ9s7p/6sWeezYfL1JVc3kBzgOeA44DzgB2AH80Lw2vwj8WTd9LfDZAepYA1zUTZ8CPLNAHZcDX5xRv7wAnDFm+VXA/UCAS4CtA79GLwPvmFV/AJcBFwE758z7HWBTN70J+OQCjzsdeL67Xd1Nr+65jiuBVd30JxeqYzmvYQ91fAL46DJeu7H5mv8zyy3/xcDuqnq+qt4APgOsn9dmPXBHN30XcEWWGgP8MFXVvqra3k1/C3gKOKvPdfRsPfDXNfIwcFqSNQOt6wrguapa7FuYvauqrwCvzps99+/gDuCaBR76U8CDVfVqVf0n8CCwrs86quqBqjrQ3X2Y0aC0g1qkP5ZjOfk6xCzDfxbw0pz7e3hz6P6vTdfprwHfP1RB3duKC4GtCyx+T5IdSe5P8sND1QAU8ECSR5NsXGD5cvqtL9cCdy6ybFb9AXBmVe3rpl8GzlygzSz7BeB6RntgC1nqNezDjd3bj9sXeRt02P3R7AG/JG8FPgfcXFWvz1u8ndGu748CfwJ8fsBSLq2qi4D3Ab+U5LIB17WoJCcA7wf+boHFs+yPQ9Ron/aIfh6d5FbgALBlkSZDv4afBt4F/BiwD/i9Pp50luHfC5wz5/7Z3bwF2yRZBZwKfLPvQpIczyj4W6rq7vnLq+r1qvqvbvo+4PgkZ/RdR/f8e7vb/cA9jHbf5lpOv/XhfcD2qnplgRpn1h+dVw6+telu9y/QZib9kuQ64Grg57p/RG+yjNdwKlX1SlX9T1V9F/jzRZ7/sPtjluF/BDg/ybndVuZa4N55be4FDh61/QDwj4t1+KS6Ywh/ATxVVb+/SJu3HTzWkORiRv00xD+hk5OccnCa0QGmnfOa3Qv8QnfU/xLgtTm7xH36IIvs8s+qP+aY+3ewAfjCAm2+BFyZZHW3G3xlN683SdYBtwDvr6pvL9JmOa/htHXMPcbzM4s8/3Lydag+jlAexpHMqxgdXX8OuLWb95uMOhfgREa7nbuBfwXOG6CGSxntRj4OPNb9XAXcANzQtbkR2MXoiOnDwHsH6o/zunXs6NZ3sE/m1hLgT7s+ewJYO0AdJzMK86lz5s2kPxj9w9kHfIfR+9SPMDrO8xDwLPAPwOld27XAbXMee333t7Ib+PAAdexm9D764N/JwU+i3g7cN+417LmOv+le+8cZBXrN/DoWy9e4H7/eKzWq2QN+UusMv9Qowy81yvBLjTL8UqMMv9Qowy816n8BuuPh7GUBDz0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "down=torch.nn.UpsamplingBilinear2d(scale_factor=1/4)\n",
    "print(train_labels.size())\n",
    "train_labels=down(train_labels.type(torch.float32))\n",
    "label = train_labels[0].squeeze()\n",
    "plt.imshow(label,cmap='gray')\n",
    "print(label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16])\n",
      "torch.Size([16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ab02acf908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "img_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.Resize([16,16]),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "labeld=img_transform(train_labels[0])\n",
    "print(labeld.size(\n",
    "))\n",
    "label = labeld[0].squeeze()\n",
    "print(label.size())\n",
    "plt.imshow(label,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation data\n",
    "valid_dataset = utils.ProstateMRDataset(partition[\"validation\"], IMAGE_SIZE)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VAE_GAN_Spade_Giulia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_gen = 0.0002\n",
    "lr_dis=0.00005\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "vae=VAE_GAN_Spade_Giulia.VAE()\n",
    "dis=VAE_GAN_Spade_Giulia.Discriminator()\n",
    "\n",
    "\n",
    "noise = torch.randn(32, 255)\n",
    "noise = noise.to(device)\n",
    "#criterion=GANLoss()\n",
    "\n",
    "\n",
    "opt_gen = torch.optim.Adam(vae.generator.parameters(), lr=LEARNING_RATE)\n",
    "opt_enc=torch.optim.Adam(vae.encoder.parameters(), lr=LEARNING_RATE)\n",
    "#opt_gen= torch.optim.Adam(gen.parameters(), lr=lr_gen)\n",
    "opt_dis= torch.optim.Adam(dis.parameters(), lr=lr_dis)\n",
    "\n",
    "TENSORBOARD_LOGDIR='vae_gan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModels():\n",
    "    torch.save(vae.generator.state_dict(), CHECKPOINTS_DIR / \"vae_enc.pth\")\n",
    "    torch.save(dis.state_dict(), CHECKPOINTS_DIR / \"vae_dis.pth\")\n",
    "    torch.save(vae.encoder.state_dict(), CHECKPOINTS_DIR / \"vae_gen.pth\")\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(opt_enc, lr_lambda)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(opt_gen, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://notebook.community/zklgame/CatEyeNets/test/GANs-PyTorch\n",
    "#https://www.researchgate.net/figure/Figure-S36-The-label-input-method-for-the-generator-in-CcGAN_fig5_348834209\n",
    "def bce_loss(input, target):\n",
    "    \"\"\"\n",
    "    Numerically stable version of the binary cross-entropy loss function.\n",
    "\n",
    "    As per https://github.com/pytorch/pytorch/issues/751\n",
    "    See the TensorFlow docs for a derivation of this formula:\n",
    "    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "\n",
    "    Inputs:\n",
    "    - input: PyTorch Variable of shape (N, ) giving scores.\n",
    "    - target: PyTorch Variable of shape (N,) containing 0 and 1 giving targets.\n",
    "\n",
    "    Returns:\n",
    "    - A PyTorch Variable containing the mean BCE loss over the minibatch of input data.\n",
    "    \"\"\"\n",
    "    #neg_abs = - input.abs()\n",
    "    #loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
    "    BCE=torch.nn.BCELoss()\n",
    "    loss=BCE(input,target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(logits_real, logits_fake):\n",
    "    \"\"\"\n",
    "    Computes the discriminator loss described above.\n",
    "    \n",
    "    Inputs:\n",
    "    - logits_real: PyTorch Variable of shape (N,) giving scores for the real data.\n",
    "    - logits_fake: PyTorch Variable of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Variable containing (scalar) the loss for the discriminator.\n",
    "    \"\"\"\n",
    "    real_loss = bce_loss(logits_real, torch.ones(BATCH_SIZE,1))\n",
    "    fake_loss = bce_loss(logits_fake, torch.zeros(BATCH_SIZE,1))\n",
    "    loss = (real_loss + fake_loss) #Have a look at this ? correct?\n",
    "    return loss\n",
    "\n",
    "def generator_loss(logits_fake):\n",
    "    \"\"\"\n",
    "    Computes the generator loss described above.\n",
    "\n",
    "    Inputs:\n",
    "    - logits_fake: PyTorch Variable of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Variable containing the (scalar) loss for the generator.\n",
    "    \"\"\"\n",
    "    loss = bce_loss(logits_fake, torch.ones(BATCH_SIZE,1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/29 [00:00<?, ?it/s]\u001b[AC:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\autograd\\__init__.py:156: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\asyncio\\base_events.py\", line 442, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\asyncio\\base_events.py\", line 1462, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\tornado\\gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2867, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-2f19c3ecc67c>\", line 20, in <module>\n",
      "    fake_img, mu, logvar  = vae(img,seg) #remove seg if you want to train without labels\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\20191679\\Documents\\GitHub\\Capita_selecta\\VAE_GAN_Spade_Giulia.py\", line 285, in forward\n",
      "    mu, logvar = self.encoder(x)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\20191679\\Documents\\GitHub\\Capita_selecta\\VAE_GAN_Spade_Giulia.py\", line 176, in forward\n",
      "    x = self.out(x)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"C:\\Users\\20191679\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\nn\\functional.py\", line 1848, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      " (Triggered internally at  ..\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:104.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "  0%|                                                                                           | 0/29 [00:09<?, ?it/s]\n",
      "  0%|                                                                                           | 0/10 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16384, 512]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2f19c3ecc67c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mopt_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mloss_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mopt_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gen_models\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16384, 512]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "img_lists = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "from torch.autograd import Variable\n",
    "\n",
    "writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR)  # tensorboard summary\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    print(f'Epoch {epoch+1}/{N_EPOCHS}')\n",
    "    for (img, seg) in tqdm(dataloader):\n",
    "        img = img.to(device)\n",
    "        seg = seg.to(device)\n",
    "\n",
    "        seg=seg.float()\n",
    "\n",
    "        \n",
    "        fake_img, mu, logvar  = vae(img,seg) #remove seg if you want to train without labels\n",
    "        \n",
    "        pred_fake = dis(fake_img,seg) \n",
    "        \n",
    "\n",
    "        # Real Detection and Loss\n",
    "       \n",
    "        pred_real = dis(img,seg)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        loss_enc = VAE_GAN_Spade_Giulia.vae_loss(img, fake_img, mu, logvar)\n",
    "        \n",
    "\n",
    "\n",
    "        opt_enc.zero_grad()\n",
    "\n",
    "        loss_enc.backward(retain_graph=True)\n",
    "\n",
    "        opt_enc.step()\n",
    "        \n",
    "        #torch.autograd.set_detect_anomaly(True)\n",
    "        \n",
    "        \n",
    "        loss_G =  generator_loss(pred_fake)+loss_enc\n",
    "        G_losses.append(loss_G.detach().cpu())\n",
    "        \n",
    "        opt_gen.zero_grad()\n",
    "        \n",
    "        loss_G.backward(retain_graph=True)\n",
    "        \n",
    "        opt_gen.step()\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #dis.trainable=True\n",
    "       \n",
    "\n",
    "        loss_D = discriminator_loss(pred_real,pred_fake)\n",
    "        #loss_fake=adversarial_loss(pred_fake,seg)        \n",
    "        \n",
    "        D_losses.append(loss_D.detach().cpu())\n",
    "        \n",
    "        opt_dis.zero_grad()\n",
    "        loss_D.backward()\n",
    "        opt_dis.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #dis.trainable = False\n",
    "        \n",
    "\n",
    "        \n",
    "        # if i%200 == 0:\n",
    "        #     print(\"Iteration {}/{} started\".format(i+1, len(dataloader)))\n",
    "    writer.add_scalar(\"Loss dis\", loss_D / len(dataloader), epoch)\n",
    "    writer.add_scalar(\"Loss gen\", loss_G / len(dataloader), epoch)  \n",
    "    scheduler.step()   \n",
    "    print(\"G_loss= \",loss_G, \" : D_Loss= \", loss_D)\n",
    "\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0: #This is for the tensorboard\n",
    "        vae.generator().eval()\n",
    "        img_grid = make_grid(\n",
    "            torch.cat((fake_img[:5], img[:5])), nrow=5, padding=12, pad_value=-1\n",
    "        )\n",
    "        writer.add_image(\n",
    "            \"Real/fake_recon\",\n",
    "            np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5,\n",
    "            epoch + 1,\n",
    "        )\n",
    "        \n",
    "    if epoch%2 == 0:\n",
    "        with torch.no_grad():\n",
    "            img_lists.append(fake_img.detach().cpu().numpy())\n",
    "    #Plot results between epochs to see if it generates something\n",
    "    if epoch%2==0:\n",
    "        for (num,_) in enumerate(img_grid):\n",
    "            print(num)\n",
    "            img = img_grid[num].squeeze()\n",
    "            #label = train_labels[num].squeeze()\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "            #plt.imshow(label,cmap='gray',alpha=0.3)\n",
    "            plt.show()\n",
    "    saveModels()\n",
    "                \n",
    "    iters+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='GAN_runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training loop\n",
    "# writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR)  # tensorboard summary\n",
    "# loss_train=[]\n",
    "# img_list_vae=[]\n",
    "# for epoch in range(N_EPOCHS):\n",
    "#     current_train_loss = 0.0\n",
    "#     current_valid_loss = 0.0\n",
    "\n",
    "#     for x_real, mask in tqdm(dataloader,position=0):\n",
    "#         # needed to zero gradients in each iteration\n",
    "#         optimizer.zero_grad()\n",
    "#         x_recon, mu, logvar = vae_model(x_real,mask)  # forward pass\n",
    "#         loss = vae.vae_loss(x_real, x_recon, mu, logvar)\n",
    "#         loss_train.append(loss)\n",
    "#         current_train_loss += loss.item()\n",
    "#         loss.backward()  # backpropagate loss\n",
    "#         optimizer.step()  # update weights\n",
    "\n",
    "#     # write to tensorboard log\n",
    "#     print(\"train loss= \", loss_train[epoch])\n",
    "#     writer.add_scalar(\"Loss/train\", loss_train[epoch], epoch)\n",
    "\n",
    "#     scheduler.step()  # step the learning step scheduler\n",
    "\n",
    "#     #save examples of real/fake images\n",
    "#     if (epoch + 1) % 1 == 0:\n",
    "#         vae_model.eval()\n",
    "#         img_grid = make_grid(\n",
    "#             torch.cat((x_recon[:5], x_real[:5])), nrow=5, padding=12, pad_value=-1\n",
    "#         )\n",
    "#         writer.add_image(\n",
    "#             \"Real/fake_recon\",\n",
    "#             np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5,\n",
    "#             epoch + 1,\n",
    "#         )\n",
    "\n",
    "#         noise = torch.randn(32, Z_DIM)\n",
    "#         image_samples = vae_model.generator(noise,mask)\n",
    "#         img_grid = make_grid(\n",
    "#             torch.cat((image_samples[:5], image_samples[5:])),\n",
    "#             nrow=5,\n",
    "#             padding=12,\n",
    "#             pad_value=-1,\n",
    "#         )\n",
    "#         writer.add_image(\n",
    "#             \"Samples\",\n",
    "#             np.clip(img_grid[0][np.newaxis], -1, 1) / 2 + 0.5,\n",
    "#             epoch + 1,\n",
    "#         )\n",
    "#     if epoch%2 == 0:\n",
    "#         with torch.no_grad():\n",
    "#             img_list_vae.append(image_samples.detach().cpu().numpy())\n",
    "#     #Plot results between epochs to see if it generates something\n",
    "#     if epoch%2==0:\n",
    "#         for i in range(25):\n",
    "#             plt.subplot(5, 5, i+1)\n",
    "#             img=img_list_vae[0].squeeze()\n",
    "#             #print(img.shape)\n",
    "#             plt.imshow(img[i], interpolation='nearest', cmap='gray_r')\n",
    "#             plt.axis('off')\n",
    "#         plt.tight_layout()\n",
    "#         plt.suptitle('Epoch {}'.format(epoch))\n",
    "#         plt.show()\n",
    "#         saveModels(epoch)\n",
    "\n",
    "#     vae_model.train()\n",
    "\n",
    "\n",
    "# torch.save(vae_model.state_dict(), CHECKPOINTS_DIR / \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='vae_sp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "img_size=[8,8]\n",
    "mr_image_list=[]\n",
    "\n",
    "\n",
    "img_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "# standardise intensities based on mean and std deviation\n",
    "train_data_mean = np.mean(mr_image_list)\n",
    "train_data_std = np.std(mr_image_list)\n",
    "norm_transform = transforms.Normalize(\n",
    "    train_data_mean, train_data_std\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
